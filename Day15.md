# Day15 朴素贝叶斯分类器和黑盒机器学习
## 朴素贝叶斯分类器  
* 先验概率与后验概率  

先验概率：指根据以往经验和分析得到的概率，如全概率公式，它往往作为“由因求果”问题中的”因”出现。  
后验概率：指依据得到“结果”信息所计算出的最有可能是哪种事件发生，是“执果寻因”问题中的“因”。  
例如：
假设我们出门堵车的可能因素有两个：车辆太多和交通事故。
堵车的概率就是先验概率 。
那么如果我们出门之前我们听到新闻说今天路上出了个交通事故，那么我们想算一下堵车的概率，这个就叫做条件概率 。也就是P(堵车|交通事故)。这是有因求果。
如果我们已经出了门，然后遇到了堵车，那么我们想算一下堵车时由交通事故引起的概率有多大，那这个就叫做后验概率 （也是条件概率，但是通常习惯这么说）。也就是P(交通事故|堵车)。这是有果求因。   

* 贝叶斯定理解决的是逆向概率问题——如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。
## 黑盒机器学习
* 特征提取：将原始的输入映射到数值数组，理想情况下，提取点的基本特征。
* ML问题分类： 分类 多分类 回归  
统计学中有两种模型：概率模型与非概率模型   
概率模型：形式为P(x|y)，即在学习过程中，y未知，训练后模型得到的输出是x的一系列值的概率；   
非概率模型：形式为决策函数，即输入x到输出y的一个映射，且输出唯一；   
软分类与硬分类问题   
软分类：使用的是概率模型，输出不同类对应的概率，最后的分类结果取概率最大的类，如多SVM组合分类；  
硬分类：使用的是非概率模型，分类结果就是决策函数的决策结果  
* 交叉验证  
目的：用于评价模型的泛化能力，从而进行模型选择
基本思想：把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对模型进行训练,再利用验证集来测试模型的泛化误差。另外，现实中数据总是有限的，为了对数据形成重用，从而提出k-折叠交叉验证   
具体流程见[链接](https://cloud.tencent.com/developer/news/238899)
