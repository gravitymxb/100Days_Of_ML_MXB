# DAY33和34 随机森林
 ## 概念
1、**随机森林**：主要用于分类与回归，是有监督的集成学习模型。它用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。得到森林后，当新的输入样本进入时，就让森林中的每一棵决策树分别进行判断，看看这个样本应该属于哪一类（对于分类算法），哪一类被选择最多，就预测这个样本为该类。   
2、 [集成学习法](https://blog.csdn.net/qq_30189255/article/details/51532442):将多个分类方法聚集在一起以提高分类的准确性。包括bagging法与boosting法。 随机森林是基于bagging框架下的决策树模型。   
## 建立随机森林
1、随机森林分类算法流程图   
![图示](https://github.com/gravitymxb/100Days_Of_ML_MXB/blob/master/33%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg)   
2、建立时注意：**采样**与**完全分裂**    
**采样**：随机森林对输入的数据进行行、列的采样。采样时采取有放回的方式，即采样得到的样本集合中可能有重复的样本。（两个随机性的引入使其不易陷入过拟合）     
**完全分裂**：对采样后的样本数据使用完全分裂的方式建立出决策树，此时的树的某个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类的。   
3、每棵树按如下方式生成：  
* （数据随机选取）如果训练集有N个样本，又放回的随机抽取n个，这个样本将是生成该决策树的训练集。
*  （特征随机选取）对于每个样本，如果有M个输入变量（或特征），指定一个常数m，然后随机地从M个特征中选取m个特征子集，然后将m个特征中最优的分裂特征用来分裂节点。      

4、所有树组合成的随机森林之所以厉害：森林中的每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。    

## 代码如下
```python
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
# day33_34  随机森林

# 导入库
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# 导入数据集
dataset = pd.read_csv('D:\python_pycharm\Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# 将数据集拆分为训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=0)


# 特征缩放
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# 调试训练集中的随机森林
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=0)
classifier.fit(X_train, y_train)

# 预测测试集结果
y_pred = classifier.predict(X_test)

# 生成混淆矩阵（误差矩阵）
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# 将训练集结果可视化
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classification (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# 将测试结果可视化
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classification (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
训练集图示：
![training set]https://github.com/gravitymxb/100Days_Of_ML_MXB/blob/master/33%20randomforest%20training%20set.png()
测试集图示：
![test set](https://github.com/gravitymxb/100Days_Of_ML_MXB/blob/master/33%20randomforest%20test%20set.png)

