# DAY33和34 随机森林
 ## 概念
1、**随机森林**：主要用于分类与回归，是有监督的集成学习模型。它用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。得到森林后，当新的输入样本进入时，就让森林中的每一棵决策树分别进行判断，看看这个样本应该属于哪一类（对于分类算法），哪一类被选择最多，就预测这个样本为该类。   
2、 [集成学习法](https://blog.csdn.net/qq_30189255/article/details/51532442):将多个分类方法聚集在一起以提高分类的准确性。包括bagging法与boosting法。 随机森林是基于bagging框架下的决策树模型。   
## 建立随机森林
1、随机森林分类算法流程图   
![图示](https://github.com/gravitymxb/100Days_Of_ML_MXB/blob/master/33%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg)   
2、建立时注意：**采样**与**完全分裂**    
**采样**：随机森林对输入的数据进行行、列的采样。采样时采取有放回的方式，即采样得到的样本集合中可能有重复的样本。（两个随机性的引入使其不易陷入过拟合）     
**完全分裂**：对采样后的样本数据使用完全分裂的方式建立出决策树，此时的树的某个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类的。
3、每棵树按如下方式生成：  
* （数据随机选取）如果训练集有N个样本，又放回的随机抽取n个，这个样本将是生成该决策树的训练集。
*  （特征随机选取）对于每个样本，如果有M个输入变量（或特征），指定一个常数m，然后随机地从M个特征中选取m个特征子集，然后将m个特征中最优的分裂特征用来分裂节点。    
4、所有树组合成的随机森林之所以厉害：森林中的每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。    

